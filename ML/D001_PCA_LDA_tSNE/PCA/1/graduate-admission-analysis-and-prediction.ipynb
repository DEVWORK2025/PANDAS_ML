{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Graduate Admission Analysis and Prediction\n\n![](https://learnrhome.files.wordpress.com/2019/05/graduate.png)\n\n![](http://debarghyadas.com/writes/assets/grad-main.png)"},{"metadata":{},"cell_type":"markdown","source":"### Purpose\n\nTo apply for a master's degree is a very expensive and intensive work. With this kernel, students will guess their capacities and they will decide whether to apply for a master's degree or not.\n\n\nSo, basically this set is about the Graduate Admissions data i.e. Given a set of standardized scores like GRE, TOEFL, SOP standard scores, LOR standard scores, what is probability ( basically i have done a YES/NO scenario ) of gaining admission into a particular school. All those folks who are preparing for MS, might point out this question, from where did you get SOP & LOR scores. These arenâ€™t public figures ? I mean yes, it might not be public, but dont you think universities might be grading these applications on some scale of rating so that the scores can be standardized. Hence the SOP, LOR scores.\n\n### Dataset\n\nThis dataset is created for prediction of graduate admissions and the dataset link is below:\n\n* Features in the dataset:\n\n* GRE Scores (290 to 340)\n\n* TOEFL Scores (92 to 120)\n\n* University Rating (1 to 5)\n\n* Statement of Purpose (1 to 5)\n\n* Letter of Recommendation Strength (1 to 5)\n\n* Undergraduate CGPA (6.8 to 9.92)\n\n* Research Experience (0 or 1)\n\n* Chance of Admit (0.34 to 0.97)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step1: Data Collection/ Data Extraction"},{"metadata":{},"cell_type":"markdown","source":"#### Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Dataset\ndata = pd.read_csv(\"../input/Admission_Predict.csv\")\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop('Serial No.', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.rename({'Chance of Admit ': 'Chance of Admit', 'LOR ':'LOR'}, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2:  Data Analysis or Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see top 10 observation row and column wise\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the detail information of dataset\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## General statistics of the data\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Correlation coeffecients heatmap\nsns.heatmap(data.corr(), annot=True).set_title('Correlation Factors Heat Map', color='black', size='20')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### GRE Scores\n\nThe Graduate Record Examinations (GRE) is a the most popular test for graduate schools' admission, it consists of three sections : Analytical Writing, Verbal and Quantitative.\n\nThe test's maximum score is 340 and minimum is 260, and according to an official GRE score document, the mean test score for all individuals from July 1,2014 to June 30,2017 (almost 1,700,000 test taker) is 306.35 which rounds to 306 with an average standard deviation of 7.19\n\nsource: https://www.ets.org/s/gre/pdf/gre_interpreting_scores.pdf"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolating GRE Score data\nGRE = pd.DataFrame(data['GRE Score'])\nGRE.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sample's GRE score mean is 316 which is a little bit higher than the mean mentioned previously (306)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Probability Distribution\nsns.distplot(GRE).set_title('Probability Distribution for GRE Test Scores', size='20')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the distribution plot shows, the GRE test scores are somehow normally distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Coeffecients for GRE Score Test\nGRE_CORR = pd.DataFrame(data.corr()['GRE Score'])\nGRE_CORR.drop('GRE Score', axis=0, inplace=True)\nGRE_CORR.rename({'GRE Score': 'GRE Correlation Coeff'}, axis=1, inplace=True)\nGRE_CORR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### TOEFL Score\n\nTest of English as a Foreign Language (TOEFL) is a very popular test for English language amongst universities worldwide, it is marked based on three sections: Reading, Listening, Speaking, and Writing, each one of them is out of 30, yielding a maximum score of 120 and a minimum of 0.\n\nETS (the institute that offers the test) recorded a mean score of 82.6 with a standard deviation of 19.5 (https://www.ets.org/s/toefl/pdf/94227_unlweb.pdf).\n\nAlthough this is the mean for a wide range of students from all around the world that took the test for different purposes, as students applying for an engineering graduate degree might have a higher average than high school students."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolating and describing TOEFL Score\nTOEFL = pd.DataFrame(data['TOEFL Score'], columns=['TOEFL Score'])\nTOEFL.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's evident that the sample over performs in the TOEFL."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Probability distribution for TOEFL Scores\nsns.distplot(TOEFL).set_title('Probability Distribution for TOEFL Scores', size='20')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### CGPA\nCumulative Grade Points Average (CGPA) is a measure of a student's marks thus his performance in his undergraduate degree."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolating and describing the CGPA\nCGPA = pd.DataFrame(data['CGPA'], columns=['CGPA'])\nCGPA.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(CGPA).set_title('Probability Distribution Plot for CGPA', size='20')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Research"},{"metadata":{"trusted":true},"cell_type":"code","source":"RES_Count = data.groupby(['Research']).count()\nRES_Count = RES_Count['GRE Score']\nRES_Count = pd.DataFrame(RES_Count)\nRES_Count.rename({'GRE Score': 'Count'}, axis=1, inplace=True)\nRES_Count.rename({0: 'No Research', 1:'Research'}, axis=0, inplace=True)\nplt.pie(x=RES_Count['Count'], labels=RES_Count.index, autopct='%1.1f%%')\nplt.title('Research', pad=5, size=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### University Rating\nThe rating of the university the student completed his undergraduate degree from."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolating and describing \nUniversity_Rating = data.groupby(['University Rating']).count()\nUniversity_Rating = University_Rating['GRE Score']\nUniversity_Rating = pd.DataFrame(University_Rating)\nUniversity_Rating.rename({'GRE Score': 'Count'}, inplace=True, axis=1)\nUniversity_Rating","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How good the university is a value between 1 and 5 in integer increment , and since it has positive correlation factors with other variables it's clear that 5 is the highest rating and 1 is the lowest."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Barplot for the distribution of the University Rating\nsns.barplot(University_Rating.index, University_Rating['Count']).set_title('University Rating', size='20')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### SOP\nStatement of Purpose (SOP) is a letter written by the student himself to state his purpose and motivation for completing a graduate degree in addition to his goals while and after he completes his study. Many universities find this letter significant because it better describe the student from a personal perspective."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Isolating and describing\nSOP = pd.DataFrame(data.groupby(['SOP']).count()['GRE Score'])\nSOP.rename({'GRE Score':'Count'}, axis=1, inplace=True)\nSOP","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Barplot for SOP \nsns.barplot(SOP.index, SOP['Count']).set_title('Statement of Purpose', size='20')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploring this variable, it's ordered from 1 to 5 with 0.5 increments, although the criteria for assessment isn't specified, which will make it harder to deal with new entries."},{"metadata":{},"cell_type":"markdown","source":"##### LOR\nLetter of Recommendation (LOR) is a letter written by a person that knows the student and recommends that the university accept his admission, this person can be a professor in his undergraduate degree or a professional whom the student have worked with."},{"metadata":{"trusted":true},"cell_type":"code","source":"LOR = pd.DataFrame(data.groupby(['LOR']).count()['GRE Score'])\nLOR.rename({'GRE Score':'Count'}, axis=1, inplace=True)\nLOR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"THE LOR is ordered same as the SOP"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the LOR\nsns.barplot(LOR.index, LOR['Count']).set_title('Letter of Recommendation', size='20')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are students in this sample too good?\n\nFirst looking at the students LOR, SOP and University Ratings, most students score (4-5) on the scales, not many scored (1-2.5).\n\nNonetheless, comparing means for their GRE and TOEFL scores, which are of a universal criteria, they clearly perform better than the average student as the ETS states.\n\nTo conclude, it seems only wise to consider the sample to be somehow above average."},{"metadata":{},"cell_type":"markdown","source":"##### Chance of Admission\n\nLet's first take a review on the chances of admission."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Chance of Admit']\nsns.distplot(data['Chance of Admit']).set_title('Probability Distribution of Chance of Admit', size='20')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()['Chance of Admit']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And since one of our goals is to predict the chance of admission, let's take a look on how the different variables correlate with it."},{"metadata":{"trusted":true},"cell_type":"code","source":"COA_corr = pd.DataFrame(data.corr()['Chance of Admit'])\nCOA_corr.rename({'Chance of Admit': 'Correlation Coeffecient'}, axis=1, inplace=True)\nCOA_corr.drop('Chance of Admit', inplace=True)\nCOA_corr.sort_values(['Correlation Coeffecient'], ascending=False, inplace=True)\nCOA_corr_x = COA_corr.index\nCOA_corr_y = COA_corr['Correlation Coeffecient']\nsns.barplot(y=COA_corr_x,x=COA_corr_y).set_title('Chance of Admit Correlation Coeffecients', size='20')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is evident that the most contribution factors to the chance of admission are CGPA, GRE Score and TOEFL Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"COA_corr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction Models : Regression Algorithm (Supervised Machine Learning)\n\n1. Linear Regression\n2. Decision Tree\n3. Random Forest"},{"metadata":{},"cell_type":"markdown","source":"### Step 3: Train Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(['Chance of Admit'], axis=1)\ny = data['Chance of Admit']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standardization\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX[['CGPA','GRE Score', 'TOEFL Score']] = scaler.fit_transform(X[['CGPA','GRE Score', 'TOEFL Score']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Linear Regression (All Features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step4 : Test Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({\"Actual\": y_test, \"Predict\": y_test}).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score, mean_squared_error\nlr_r2 = r2_score(y_test, y_pred)\nlr_mse = mean_squared_error(y_test, y_pred)\nlr_rmse = np.sqrt(lr_mse)\nprint('Linear Regression R2 Score: {0} \\nLinear Regression MSE: {1}, \\nLinear Regression RMSE:{2}'.format(lr_r2, lr_mse, lr_rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nsns.distplot((y_test - y_pred))\nplt.title('Linear Regression (All Features) Residuals', fontdict={'fontsize':20}, pad=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\n# sns.(y_test, y_pred)\nsns.scatterplot(y_test, y_pred)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression (Selected Features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_selected = X[['CGPA', 'GRE Score', 'TOEFL Score']]\nX_sel_train, X_sel_test, y_train, y_test = train_test_split(X_selected, y, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_sel = LinearRegression()\nlr_sel.fit(X_sel_train, y_train)\nlr_sel_predictions = lr_sel.predict(X_sel_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_sel_r2 = r2_score(y_test, lr_sel_predictions)\nlr_sel_mse = mean_squared_error(y_test, lr_sel_predictions)\nlr_sel_rmse = np.sqrt(lr_sel_mse)\nprint('Linear Regression R2 Score: {0} \\nLinear Regression MSE: {1}, \\nLinear Regression RMSE:{2}'.format(lr_sel_r2, lr_sel_mse, lr_sel_rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators = 100, random_state = 101)\nrfr.fit(X_train,y_train)\ny_head_rfr = rfr.predict(X_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test, y_head_rfr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Decision Tree "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor(random_state = 101)\ndtr.fit(X_train,y_train)\ny_head_dtr = dtr.predict(X_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test,y_head_dtr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.array([r2_score(y_test,y_pred),r2_score(y_test,y_head_rfr),r2_score(y_test,y_head_dtr)])\nx = [\"LinearRegression\",\"RandomForestReg.\",\"DecisionTreeReg.\"]\nplt.bar(x,y)\nplt.title(\"Comparison of Regression Algorithms\")\nplt.xlabel(\"Regressor\")\nplt.ylabel(\"r2_score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"red = plt.scatter(np.arange(0,80,5),y_pred[0:80:5],color = \"red\")\ngreen = plt.scatter(np.arange(0,80,5),y_head_rfr[0:80:5],color = \"green\")\nblue = plt.scatter(np.arange(0,80,5),y_head_dtr[0:80:5],color = \"blue\")\nblack = plt.scatter(np.arange(0,80,5),y_test[0:80:5],color = \"black\")\nplt.title(\"Comparison of Regression Algorithms\")\nplt.xlabel(\"Index of Candidate\")\nplt.ylabel(\"Chance of Admit\")\nplt.legend((red,green,blue,black),('LR', 'RFR', 'DTR', 'REAL'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comment:\n\nBecause most candidates in the data have over 70% chance, many unsuccessful candidates are not well predicted"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Chance of Admit\"].plot(kind = 'hist',bins = 200,figsize = (6,6))\nplt.title(\"Chance of Admit\")\nplt.xlabel(\"Chance of Admit\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction Models : Classification Algorithm (Supervised Machine Learning)\n\n1. Logistic Regression\n2. Support Vector Machine\n3. Gaussian Naive Bayes\n4. Decision Tree Classification\n5. Random Forest Classification\n6. K Nearest Neighbors (KNN) Classification"},{"metadata":{},"cell_type":"markdown","source":"### Preparing Data for Classification"},{"metadata":{},"cell_type":"markdown","source":"If a candidate's Chance of Admit is greater than 80%, the candidate will receive the 1 label.\n\nIf a candidate's Chance of Admit is less than or equal to 80%, the candidate will receive the 0 label."},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the dataset\ndf = pd.read_csv(\"../input/Admission_Predict.csv\")\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it may be needed in the future.\nserialNo = df[\"Serial No.\"].values\ndf.drop([\"Serial No.\"],axis=1,inplace = True)\n\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop([\"Chance of Admit\"],axis=1)\ny = df[\"Chance of Admit\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX = MinMaxScaler(feature_range=(0, 1))\nX_train[X_train.columns] = scalerX.fit_transform(X_train[X_train.columns])\nX_test[X_test.columns] = scalerX.transform(X_test[X_test.columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_01 = [1 if each > 0.8 else 0 for each in y_train]\ny_test_01  = [1 if each > 0.8 else 0 for each in y_test]\n\n# list to array\ny_train_01 = np.array(y_train_01)\ny_test_01 = np.array(y_test_01)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Logistic Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression()\nlogr.fit(X_train,y_train_01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predlogr = logr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy Score:\", accuracy_score(y_predlogr, y_test_01))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test_01,y_predlogr))\ncm_lrc = confusion_matrix(y_test_01,y_predlogr)\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_lrc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01, y_predlogr))\nprint(\"recall_score: \", recall_score(y_test_01, y_predlogr))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01, y_predlogr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Test for Train Dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_lrc_train = confusion_matrix(y_train_01,logr.predict(X_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_lrc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(X_train,y_train_01)\ny_pred_svm = svm.predict(X_test)\nprint(\"score: \", svm.score(X_test,y_test_01))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_svm = confusion_matrix(y_test_01,y_pred_svm)\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\ncm_svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_svm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01, y_pred_svm))\nprint(\"recall_score: \", recall_score(y_test_01,y_pred_svm))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01, y_pred_svm))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test for Train Dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_svm_train = confusion_matrix(y_train_01, svm.predict(X_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_svm_train, annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train,y_train_01)\ny_pred_nb = nb.predict(X_test)\nprint(\"score: \", nb.score(X_test,y_test_01))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_nb = confusion_matrix(y_test_01, y_pred_nb)\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\ncm_nb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_nb,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01, y_pred_nb))\nprint(\"recall_score: \", recall_score(y_test_01,y_pred_nb))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01, y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test for Train Dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_nb_train = confusion_matrix(y_train_01,nb.predict(X_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_nb_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Decision Tree Classification "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train,y_train_01)\ny_pred_dtc = dtc.predict(X_test)\nprint(\"score: \", dtc.score(X_test,y_test_01))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_dtc = confusion_matrix(y_test_01, y_pred_dtc)\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\ncm_dtc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_dtc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01, y_pred_dtc))\nprint(\"recall_score: \", recall_score(y_test_01, y_pred_dtc))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01, y_pred_dtc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test for Train Dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_dtc_train = confusion_matrix(y_train_01,dtc.predict(X_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_dtc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Random Forest Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrfc.fit(X_train,y_train_01)\n\ny_pred_rfc = rfc.predict(X_test)\n\nprint(\"score: \", rfc.score(X_test, y_test_01))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_rfc = confusion_matrix(y_test_01, y_pred_rfc)\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\ncm_rfc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_rfc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01, y_pred_rfc))\nprint(\"recall_score: \", recall_score(y_test_01, y_pred_rfc))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01, y_pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test for Train Dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_rfc_train = confusion_matrix(y_train_01, rfc.predict(X_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_rfc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. K Nearest Neighbors Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n# finding k value\nscores = []\nfor each in range(1,50):\n    knn_n = KNeighborsClassifier(n_neighbors = each)\n    knn_n.fit(X_train, y_train_01)\n    scores.append(knn_n.score(X_test, y_test_01))\n    \nplt.plot(range(1,50),scores)\nplt.xlabel(\"k\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(X_train, y_train_01)\n\ny_pred_knn = knn.predict(X_test)\nprint(\"score of 3 :\",knn.score(X_test,y_test_01))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_knn = confusion_matrix(y_test_01, y_pred_knn)\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\ncm_knn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_knn,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01, y_pred_knn))\nprint(\"recall_score: \", recall_score(y_test_01, y_pred_knn))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01, y_pred_knn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test for Train Dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_knn_train = confusion_matrix(y_train_01,knn.predict(X_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_knn_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparison of Classification Algorithms\n\nAll classification algorithms achieved around 90% success. The most successful one is Gaussian Naive Bayes with 96% score."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.array([logr.score(X_test, y_test_01), svm.score(X_test, y_test_01), nb.score(X_test, y_test_01), dtc.score(X_test,y_test_01), rfc.score(X_test, y_test_01), knn.score(X_test, y_test_01)])\n#x = [\"LogisticRegression\",\"SVM\",\"GaussianNB\",\"DecisionTreeClassifier\",\"RandomForestClassifier\",\"KNeighborsClassifier\"]\nx = [\"LogisticReg.\", \"SVM\", \"GNB\", \"Dec.Tree\", \"Ran.Forest\", \"KNN\"]\n\nplt.bar(x,y)\nplt.title(\"Comparison of Classification Algorithms\")\nplt.xlabel(\"Classfication\")\nplt.ylabel(\"Score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CLUSTERING ALGORITHMS (UNSUPERVISED MACHINE LEARNING ALGORITHMS)\n"},{"metadata":{},"cell_type":"markdown","source":"#### Preparing Data for Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/Admission_Predict.csv\")\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.rename(columns = {'Chance of Admit ':'ChanceOfAdmit'})\nserial = data[\"Serial No.\"]\ndata.drop([\"Serial No.\"],axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (data - np.min(data))/(np.max(data)-np.min(data))\nX = data.drop([\"ChanceOfAdmit\"],axis=1)\ny = data.ChanceOfAdmit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Preparing Data for Clustering (PCA)"},{"metadata":{},"cell_type":"markdown","source":"* All features (x) were collected in one feature with Principal Component Analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# for data visualization\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 1, whiten= True )  # whitten = normalize\npca.fit(X)\nx_pca = pca.transform(X)\nx_pca = x_pca.reshape(400,)\ndictionary = {\"x\":x_pca,\"y\":y}\ndata1 = pd.DataFrame(dictionary)\nprint(\"data:\")\nprint(data1.head())\nprint(\"\\ndata:\")\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. K-means Clustering"},{"metadata":{},"cell_type":"markdown","source":"* The elbow method is used to determine the best number of clusters for k-means clustering. The number is 3."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Serial No.\"] = serial\nfrom sklearn.cluster import KMeans\nwcss = []\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,15),wcss)\nplt.xlabel(\"k values\")\nplt.ylabel(\"WCSS\")\nplt.show()\n\nkmeans = KMeans(n_clusters=3)\nclusters_knn = kmeans.fit_predict(X)\n\ndata[\"label_kmeans\"] = clusters_knn\n\n\nplt.scatter(data[data.label_kmeans == 0 ][\"Serial No.\"], data[data.label_kmeans == 0].ChanceOfAdmit,color = \"red\")\nplt.scatter(data[data.label_kmeans == 1 ][\"Serial No.\"], data[data.label_kmeans == 1].ChanceOfAdmit,color = \"blue\")\nplt.scatter(data[data.label_kmeans == 2 ][\"Serial No.\"], data[data.label_kmeans == 2].ChanceOfAdmit,color = \"green\")\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"Candidates\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()\n\ndata[\"label_kmeans\"] = clusters_knn\nplt.scatter(data1.x[data.label_kmeans == 0 ],data1[data.label_kmeans == 0].y,color = \"red\")\nplt.scatter(data1.x[data.label_kmeans == 1 ],data1[data.label_kmeans == 1].y,color = \"blue\")\nplt.scatter(data1.x[data.label_kmeans == 2 ],data1[data.label_kmeans == 2].y,color = \"green\")\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Hierarchical Clustering"},{"metadata":{},"cell_type":"markdown","source":"* The dendrogram method is used to determine the best number of clusters for hierarchical clustering. The number is 3 again."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Serial No.\"] = serial\n\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nmerg = linkage(X, method=\"ward\")\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()\n\nfrom sklearn.cluster import AgglomerativeClustering\nhiyerartical_cluster = AgglomerativeClustering(n_clusters = 3, affinity= \"euclidean\", linkage = \"ward\")\nclusters_hiyerartical = hiyerartical_cluster.fit_predict(X)\n\ndata[\"label_hiyerartical\"] = clusters_hiyerartical\n\nplt.scatter(data[data.label_hiyerartical == 0 ][\"Serial No.\"],data[data.label_hiyerartical == 0].ChanceOfAdmit,color = \"red\")\nplt.scatter(data[data.label_hiyerartical == 1 ][\"Serial No.\"],data[data.label_hiyerartical == 1].ChanceOfAdmit,color = \"blue\")\nplt.scatter(data[data.label_hiyerartical == 2 ][\"Serial No.\"],data[data.label_hiyerartical == 2].ChanceOfAdmit,color = \"green\")\nplt.title(\"Hierarchical Clustering\")\nplt.xlabel(\"Candidates\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()\n\nplt.scatter(data1[data.label_hiyerartical == 0 ].x,data1.y[data.label_hiyerartical == 0],color = \"red\")\nplt.scatter(data1[data.label_hiyerartical == 1 ].x,data1.y[data.label_hiyerartical == 1],color = \"blue\")\nplt.scatter(data1[data.label_hiyerartical == 2 ].x,data1.y[data.label_hiyerartical == 2],color = \"green\")\nplt.title(\"Hierarchical Clustering\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comparison of Clustering Algorithms\n\nK-means Clustering and Hierarchical Clustering are similarly."},{"metadata":{},"cell_type":"markdown","source":"### THE THREE IMPORTANT FEATURES"},{"metadata":{},"cell_type":"markdown","source":"#### Correlation between All Columns\n\nThe 3 most important features for admission to the Master: CGPA, GRE SCORE, and TOEFL SCORE\n\nThe 3 least important features for admission to the Master: Research, LOR, and SOP"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(data.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The Three Features for Linear Regression"},{"metadata":{},"cell_type":"markdown","source":"* The first results for Linear Regression (7 features): \nr_square score: 0.821208259148699\n\n* The results for Linear Regression now (3 features):\nr_square score: 0.8212241793299223\n\n* The two results are very close. If these 3 features (CGPA, GRE SCORE, and TOEFL SCORE) are used instead of all 7 features together, the result is not bad and performance is increased because less calculation is required."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/Admission_Predict.csv')\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})\n\nnewDF = pd.DataFrame()\nnewDF[\"GRE Score\"] = df[\"GRE Score\"]\nnewDF[\"TOEFL Score\"] = df[\"TOEFL Score\"]\nnewDF[\"CGPA\"] = df[\"CGPA\"]\nnewDF[\"Chance of Admit\"] = df[\"Chance of Admit\"]\n\nx_new = df.drop([\"Chance of Admit\"],axis=1)\ny_new = df[\"Chance of Admit\"].values\n\n# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\nx_train_new, x_test_new, y_train_new, y_test_new = train_test_split(x_new, y_new, test_size = 0.20, random_state = 42)\n\n# normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX = MinMaxScaler(feature_range=(0, 1))\nx_train_new[x_train_new.columns] = scalerX.fit_transform(x_train_new[x_train_new.columns])\nx_test_new[x_test_new.columns] = scalerX.transform(x_test_new[x_test_new.columns])\n\nfrom sklearn.linear_model import LinearRegression\nlr_new = LinearRegression()\nlr_new.fit(x_train_new, y_train_new)\ny_head_lr_new = lr_new.predict(x_test_new)\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test_new, y_head_lr_new))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}